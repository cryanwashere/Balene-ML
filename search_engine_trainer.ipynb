{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ],
      "metadata": {
        "id": "lmbxGyodgQPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j9P0Tnu-b77I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3186d6c-b02e-4c89-c297-c3868f4b6602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Search-Engine'...\n",
            "remote: Enumerating objects: 918, done.\u001b[K\n",
            "remote: Counting objects: 100% (918/918), done.\u001b[K\n",
            "remote: Compressing objects: 100% (902/902), done.\u001b[K\n",
            "remote: Total 918 (delta 13), reused 910 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (918/918), 8.13 MiB | 16.51 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cryanwashere/Search-Engine.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Search-Engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBqiLDD-GYPJ",
        "outputId": "3db57178-32b3-4a04-e8e1-9afb0fce3382"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Search-Engine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_urls.py"
      ],
      "metadata": {
        "id": "tTyGad24chr_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/images\n",
        "!mkdir data/pages"
      ],
      "metadata": {
        "id": "0CWiM5IuGrJk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python download_data.py"
      ],
      "metadata": {
        "id": "LippwR9aG7Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Multi-Modal model"
      ],
      "metadata": {
        "id": "TYBr4WGsgSpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "TZpIIIxHgMjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677b461f-b51d-4028-e722-2cc56ad2c249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "azhYywsKEir-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0151b0c-acee-4c55-f1b1-62e2b77acb03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "BnKh_us1crRb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cpu = torch.device(\"cpu\")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA1aFNJbH9nz",
        "outputId": "962388fe-4560-4234-9540-68ccc7235f1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  def __init__(self, dim, vocab_size, max_seq_len):\n",
        "    super(TextEncoder, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "    self.positional_embedding = nn.Parameter(torch.zeros(1, max_seq_len, dim))\n",
        "  \n",
        "  def forward(self, x):\n",
        "\n",
        "    # x should be batches of tokens\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = x + self.positional_embedding[:,:x.shape[1],:]\n",
        "\n",
        "    return x\n",
        "\n",
        "sample_model = TextEncoder(64, 10, 5)\n",
        "\n",
        "sample_tokens = torch.tensor([[1,2,3,4]])\n",
        "sample_model(sample_tokens).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Gmjjnogq8k",
        "outputId": "81abe4fc-9502-4948-9e26-feed72f2c9fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  def __init__(self, dim, input_size, patch_size):\n",
        "    super(ImageEncoder, self).__init__()\n",
        "\n",
        "    self.dim = dim\n",
        "\n",
        "    scale = dim ** -0.5\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "    self.positional_embedding = nn.Parameter(scale * torch.randn((input_size // patch_size) ** 2, dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x.shape:\n",
        "    # [ batch_size, num_images, rgb, width, height ]\n",
        "\n",
        "    batch_size, num_images, rgb, width, height = x.shape\n",
        "    \n",
        "    x = x.reshape(batch_size * num_images, rgb, width, height) \n",
        "\n",
        "    # x.shape:\n",
        "    # [ batch_size * num_images, rgb, width, height ]\n",
        "\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    batch_image_prod, dim, grid_w, grid_h = x.shape\n",
        "\n",
        "    x = x.reshape(batch_size, num_images, dim, grid_w, grid_h)\n",
        "\n",
        "    # x.shape:\n",
        "    # [ batch_size, num_images, dim, grid_size, grid_size ]\n",
        "    \n",
        "    x = x.reshape(batch_size, num_images, self.dim, -1)\n",
        "\n",
        "    # x.shape:\n",
        "    # [ batch_size, num_images, dim, grid_size ** 2 ]\n",
        "\n",
        "    x = x.permute(0, 1, 3, 2)\n",
        "\n",
        "    # x.shape:\n",
        "    # [ batch_size, num_images, grid_size ** 2, dim ]\n",
        "\n",
        "    # the position of each image in the web page is irrelevant, however\n",
        "    # this allows the model to encode the position of each patch \n",
        "    # on each image \n",
        "\n",
        "    x = x + self.positional_embedding\n",
        "\n",
        "    batch_size, num_images, image_grid, dim = x.shape\n",
        "\n",
        "    x = x.reshape(batch_size, num_images * image_grid, dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "sample_model = ImageEncoder(64, 224, 32)\n",
        "\n",
        "sample_input = torch.zeros(8, 4, 3, 224, 224)\n",
        "sample_model(sample_input).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAcTYHgYawou",
        "outputId": "b5cfc084-dbfd-42e9-c135-d6dd7ffb8ae4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 196, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WebPageFeatureExtractor(nn.Module):\n",
        "  def __init__(self, dim, num_heads, num_layers, hidden_dim, dropout, max_text_seq_len, image_size, vocab_size):\n",
        "    super(WebPageFeatureExtractor, self).__init__()\n",
        "\n",
        "    self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(dim, num_heads, hidden_dim, dropout),\n",
        "            num_layers\n",
        "    )\n",
        "        \n",
        "    scale = dim ** -0.5\n",
        "    self.class_embedding = nn.Parameter(scale * torch.randn(dim))\n",
        "\n",
        "    self.img_encoder = ImageEncoder(dim, image_size, 32)\n",
        "    self.txt_encoder = TextEncoder(dim, vocab_size, max_text_seq_len)\n",
        "\n",
        "  def forward(self, img, txt):\n",
        "\n",
        "    # img:\n",
        "    # a tensor of any amount of images\n",
        "    # img.shape:\n",
        "    # [ batch_size, num_images, rgb, image_size, image_size]\n",
        "\n",
        "    img_embeddings = self.img_encoder(img)\n",
        "\n",
        "    # img_embeddings.shape:\n",
        "    # [ batch_size, grid_size ** 2, dim ]\n",
        "\n",
        "\n",
        "    # txt:\n",
        "    # a tensor of tokens\n",
        "    # txt.shape\n",
        "    # [ batch_size, seq_len ]\n",
        "\n",
        "    txt_embeddings = self.txt_encoder(txt)\n",
        "\n",
        "    # txt_embeddings.shape\n",
        "    # [ batch_size, seq_len, dim ]\n",
        "\n",
        "\n",
        "    batch_size = img_embeddings.shape[0]\n",
        "    class_embeddings = self.class_embedding + torch.zeros(batch_size, 1, self.class_embedding.shape[-1]).to(self.class_embedding.device)\n",
        "    \n",
        "    x = torch.cat(\n",
        "        [ class_embeddings, img_embeddings, txt_embeddings ],\n",
        "        dim = 1\n",
        "    )\n",
        "\n",
        "    x = self.transformer(x)\n",
        "\n",
        "    x = x[:, 0, :]\n",
        "\n",
        "    return x\n",
        "\n",
        "model = WebPageFeatureExtractor(\n",
        "    dim = 512,\n",
        "    num_heads = 8,\n",
        "    num_layers = 8,\n",
        "    hidden_dim = 1024, \n",
        "    dropout = 0.2, \n",
        "    max_text_seq_len = 256,\n",
        "    image_size = 224,\n",
        "    vocab_size = 30522\n",
        ").to(device)\n",
        "\n",
        "sample_text = torch.tensor([[1,2,3,4,5,6,7,8]]).to(device)\n",
        "sample_images = torch.zeros(1, 4, 3, 224, 224).to(device)\n",
        "\n",
        "model(sample_images, sample_text).shape\n",
        "\n",
        "print(f\"model uses {sum(p.numel() for p in model.parameters()) / 1000000} million parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0ac7EJCa4jZ",
        "outputId": "f08c9c44-f61c-4f2c-ed89-d50281c9a4c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "zxV4LytumVRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                                                       # Convert the image to a tensor\n",
        "    transforms.Resize((224, 224), antialias=True),                               # Resize the image to (224, 224)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
        "])\n",
        "\n",
        "def shuffle_tensor(tensor, dim):\n",
        "    # Generate a random permutation of indices\n",
        "    indices = torch.randperm(tensor.size(dim))\n",
        "\n",
        "    # Shuffle the tensor along the specified dimension using the indices\n",
        "    shuffled_tensor = tensor.index_select(dim, indices)\n",
        "\n",
        "    return shuffled_tensor\n",
        "\n",
        "def validate_datapoint(path):\n",
        "  with open(os.path.join(\"data/pages\",path), \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  split_data = text.split('IMAGES_DONE')\n",
        "\n",
        "  image_paths, text = split_data[0], split_data[1]\n",
        "\n",
        "\n",
        "  images_tensor = torch.ones(1, 3, 224, 224)\n",
        "\n",
        "  image_paths = image_paths.split('\\n')\n",
        "  image_paths = [path for path in image_paths if path != '']\n",
        "\n",
        "  if len(image_paths) < 6:\n",
        "    return False\n",
        "\n",
        "  if len(text.split(' ')) < 512:\n",
        "    return False\n",
        "\n",
        "  #tokens = tokenizer.encode(text, return_tensors='pt').squeeze()\n",
        "  \n",
        "  return True\n",
        "\n",
        "\n",
        "\n",
        "# Define the dataset class.\n",
        "class WebContentDataset(Dataset):\n",
        "\n",
        "    def __init__(self, paths):\n",
        "        self.paths = paths\n",
        "\n",
        "        self.max_images = 2\n",
        "        self.max_tokens = 256\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      with open(os.path.join(\"data/pages\",self.paths[idx]), \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "      split_data = text.split('IMAGES_DONE')\n",
        "\n",
        "      image_paths, text = split_data[0], split_data[1]\n",
        "\n",
        "\n",
        "      images_tensor = torch.ones(1, 3, 224, 224)\n",
        "\n",
        "      image_paths = image_paths.split('\\n')\n",
        "      image_paths = [path for path in image_paths if path != '']\n",
        "\n",
        "      random.shuffle(image_paths)\n",
        "      image_paths = image_paths[:self.max_images * 2]\n",
        "\n",
        "      for path in image_paths:\n",
        "        image_tensor = image_transform(Image.open(path)).unsqueeze(0)\n",
        "        images_tensor = torch.cat([images_tensor, image_tensor], dim=0)\n",
        "      \n",
        "      images_tensor = images_tensor[1:, :, :]\n",
        "\n",
        "      images_tensor = shuffle_tensor(images_tensor, 0)\n",
        "\n",
        "      tokens = tokenizer.encode(text, return_tensors='pt').squeeze()\n",
        "\n",
        "\n",
        "      num_tokens = tokens.shape[0]\n",
        "      num_images = images_tensor.shape[0]\n",
        "\n",
        "      assert num_tokens >= self.max_tokens * 2\n",
        "      assert num_images >= self.max_images * 2\n",
        "\n",
        "      \n",
        "      img_seq_1 = images_tensor[:self.max_images, :, :, :]\n",
        "      img_seq_2 = images_tensor[self.max_images : self.max_images * 2, :, :, :]\n",
        "\n",
        "      tok_seq_1 = tokens[:self.max_tokens]\n",
        "      tok_seq_2 = tokens[self.max_tokens : self.max_tokens * 2]\n",
        "      \n",
        "\n",
        "      return (img_seq_1, tok_seq_1), (img_seq_2, tok_seq_2)\n",
        "\n",
        "file_paths = os.listdir(\"data/pages\")\n",
        "file_paths = [path for path in file_paths if validate_datapoint(path)]\n",
        "ds = WebContentDataset(file_paths)\n",
        "len(ds)#, ds[0]"
      ],
      "metadata": {
        "id": "349NrBykmYFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0bc1be-dcf9-45b2-e133-efc58c350dd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "574"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_datapoint(images, tokens):\n",
        "\n",
        "\n",
        "  # Convert the tensor to numpy array\n",
        "  images = images.transpose(1,3)\n",
        "\n",
        "   # Apply softmax to convert logits to probabilities\n",
        "  #images_probs = torch.sigmoid(images)\n",
        "\n",
        "  # Scale the probabilities to a range of [0, 1]\n",
        "  #images_scaled = (images_probs - torch.min(images_probs)) / (torch.max(images_probs) - torch.min(images_probs))\n",
        "\n",
        "  images_np = images.numpy()\n",
        "\n",
        "  # Calculate the number of rows and columns for subplots\n",
        "  num_images = images_np.shape[0]\n",
        "  num_rows = int(num_images ** 0.5)\n",
        "  num_cols = int((num_images + num_rows - 1) / num_rows)\n",
        "\n",
        "  # Create a figure and subplots\n",
        "  fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
        "\n",
        "  # Plot each image\n",
        "  for i, ax in enumerate(axes.flat):\n",
        "      if i < num_images:\n",
        "          ax.imshow(images_np[i])\n",
        "          ax.axis('off')\n",
        "      else:\n",
        "          ax.axis('off')\n",
        "\n",
        "  # Adjust the layout and display the plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  print(tokenizer.decode(tokens))\n",
        "\n",
        "#dp_1, dp_2 = ds[3]\n",
        "\n",
        "#visualize_datapoint(*dp_1)"
      ],
      "metadata": {
        "id": "aeghIW4q6QnA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "5IsPV4VHRBdj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "parameters = list(model.parameters()) \n",
        "optim = torch.optim.Adam(parameters, lr = 0.00001)"
      ],
      "metadata": {
        "id": "zp2eRxRVRjfV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference the model, using it for search"
      ],
      "metadata": {
        "id": "AgeQzq-G4ig0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "\n",
        "    # Normalize the tensors to unit length\n",
        "    tensor1 = F.normalize(vector1, dim=0)\n",
        "    tensor2 = F.normalize(vector2, dim=0)\n",
        "\n",
        "    # Calculate the cosine similarity between the two tensors\n",
        "    similarity = F.cosine_similarity(tensor1, tensor2, dim=0)\n",
        "\n",
        "    return similarity.item()"
      ],
      "metadata": {
        "id": "pn6U1Br5DR_m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_data(path : str):\n",
        "\n",
        "  with open(path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  split_data = text.split('IMAGES_DONE')\n",
        "\n",
        "  image_paths, text = split_data[0], split_data[1]\n",
        "  \n",
        "\n",
        "  images_tensor = torch.ones(1, 3, 224, 224)\n",
        "\n",
        "  image_paths = image_paths.split('\\n')\n",
        "  \n",
        "  \n",
        "  random.shuffle(image_paths)\n",
        "  image_paths = image_paths[:2]\n",
        "\n",
        "  image_paths = filter(lambda x : x != '', image_paths)\n",
        "  for path in image_paths:\n",
        "    image_tensor = image_transform(Image.open(path)).unsqueeze(0)\n",
        "    images_tensor = torch.cat([images_tensor, image_tensor], dim=0)\n",
        "  \n",
        "  images_tensor = images_tensor[1:, :, :]\n",
        "\n",
        "  images_tensor = images_tensor.unsqueeze(0)\n",
        "  #images_tensor = shuffle_tensor(images_tensor, 0).unsqueeze(0)[:,:2,:,:,:]\n",
        "\n",
        "  tokens = tokenizer.encode(text, return_tensors='pt')[:,:256]\n",
        "\n",
        "  return images_tensor, tokens\n",
        "\n",
        "def encode_data(path : str) -> torch.tensor:\n",
        "\n",
        "  images, tokens = open_data(path)\n",
        "\n",
        "  images, tokens = images.to(device), tokens.to(device)\n",
        "\n",
        "  #print(images, tokens)\n",
        "\n",
        "  vec = model(images, tokens)\n",
        "\n",
        "  return vec\n",
        "\n",
        "\n",
        "def get_search_vecs():\n",
        "\n",
        "  search_dict = dict()\n",
        "\n",
        "  for path in search_paths:\n",
        "    vec = encode_data(path)\n",
        "\n",
        "    page_name = path.split('/')[-1]\n",
        "    search_dict[page_name] = vec\n",
        "  \n",
        "  return search_dict\n",
        "\n",
        "def get_search_scores():\n",
        "  vec_dict = get_search_vecs()\n",
        "\n",
        "  query_vec = vec_dict[\"query.txt\"].squeeze()\n",
        "\n",
        "  for key in vec_dict.keys():\n",
        "    key_vec = vec_dict[key].squeeze()\n",
        "\n",
        "    similarity = cosine_similarity(query_vec, key_vec)\n",
        "\n",
        "    vec_dict[key] = similarity\n",
        "  \n",
        "  return vec_dict\n",
        "\n",
        "\n",
        "\n",
        "search_paths = os.listdir('sample_data/pages')\n",
        "search_paths = [f\"sample_data/pages/{page}\" for page in search_paths][:5]\n",
        "search_paths.append(\"sample_data/pages/query.txt\")\n",
        "print(search_paths)\n",
        "get_search_scores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVqox1CX4hxK",
        "outputId": "527983cb-d802-408c-ebac-b9ca17dfd4c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample_data/pages/flower.txt', 'sample_data/pages/alexander_gerald.txt', 'sample_data/pages/learning_how_to_learn.txt', 'sample_data/pages/piano.txt', 'sample_data/pages/nature.txt', 'sample_data/pages/query.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10489 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'flower.txt': 0.7702924013137817,\n",
              " 'alexander_gerald.txt': 0.7663944363594055,\n",
              " 'learning_how_to_learn.txt': 0.7842341065406799,\n",
              " 'piano.txt': 0.7998403906822205,\n",
              " 'nature.txt': 0.7305622696876526,\n",
              " 'query.txt': 1.0000001192092896}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-PSLFUscuBg",
        "outputId": "0f18a9e7-7425-473f-ed41-f94240d991b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neptune in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.1.31)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune) (2.7.0)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.26.133)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.0.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.26.15)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.133 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (1.29.133)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (0.6.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (5.17.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.19.1)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.3.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.22.4)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.4)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune\n",
        "\n",
        "run = neptune.init_run(\n",
        "    project=\"cjryanwashere/Search-Engine\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2ODc4MDgwNS0wYzEwLTQ4MWEtOTJlOS0wODAxY2JmNmIzYmEifQ==\",\n",
        ")  # your credentials\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Q4C2Xec5wr",
        "outputId": "1a7e44a2-ac42-4e75-877a-bc189a7abbd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-2cc3f6c79269>:3: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
            "  run = neptune.init_run(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/cjryanwashere/Search-Engine/e/SEARCH-6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_search_scores():\n",
        "  scores = get_search_scores()\n",
        "\n",
        "  for key in scores.keys():\n",
        "    run[f\"cosine_similarity/{key}\"].append(scores[key])\n",
        "\n",
        "write_search_scores()"
      ],
      "metadata": {
        "id": "3BXcVCMgKsiz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  print(f\"epoch {epoch}\")\n",
        "  for i, batch in enumerate(dl):\n",
        "\n",
        "    seq_1, seq_2 = batch\n",
        "    (img_1, tok_1), (img_2, tok_2) = seq_1, seq_2\n",
        "    img_1, tok_1, img_2, tok_2 = img_1.to(device), tok_1.to(device), img_2.to(device), tok_2.to(device)\n",
        "\n",
        "    out_1, out_2 = model(img_1, tok_1), model(img_2, tok_2)\n",
        "    #out_1, out_2 = model(*seq_1), model(*seq_2)\n",
        "\n",
        "    similarity_matrix = out_1 @ out_2.T\n",
        "    label = torch.eye(similarity_matrix.shape[0]).to(device)\n",
        "\n",
        "    loss = criterion(similarity_matrix, label)\n",
        "    run[\"train/loss\"].append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    print(\".\",end='')\n",
        "\n",
        "    if i % 20 == 0 and i != 0:\n",
        "      write_search_scores()\n",
        "      print(\"\\n writing search scores...\")"
      ],
      "metadata": {
        "id": "nsi-6AtgSYHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebe6ee2-0f7f-4b8c-8102-b323960dd2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................................................................................................................................................"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GInLREc6LpZx",
        "outputId": "a060e43c-fc3a-4861-e595-8428a90c7efd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "All 0 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/cjryanwashere/Search-Engine/e/SEARCH-4/metadata\n"
          ]
        }
      ]
    }
  ]
}